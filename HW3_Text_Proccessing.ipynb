{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbIqu6CKBTjl",
        "outputId": "3a305353-03fb-4798-ba29-0127fbd16abc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m666.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=b407715ba8b54af21039ae3b2de214aa753fbb0fdf9c7a7d79195a8d7f5a92db\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "!pip install pymorphy2\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "stop_wordsR = set(stopwords.words('russian'))\n",
        "stop_wordsE = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "X_train = train_df[\"title\"].values\n",
        "X_test = test_df[\"title\"].values\n",
        "y_train = train_df[\"label\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2EkwXrZ_zvl",
        "outputId": "17795e04-0aed-468d-bab2-e027a6a9f51b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16715\n"
          ]
        }
      ],
      "source": [
        "train_df.head(30)\n",
        "print(y_train.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGxZZQmCcsZE"
      },
      "outputs": [],
      "source": [
        "#!pip install FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Векторизация"
      ],
      "metadata": {
        "id": "mMajIYZ-cFKx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KzVfR2QRG7fX"
      },
      "outputs": [],
      "source": [
        "text=np.copy(X_train)\n",
        "\n",
        "y_tr=np.copy(y_train)\n",
        "urld = train_df[\"url\"].values\n",
        "\n",
        "text=np.where(text==None,'0',text)\n",
        "text=[str(i) for i in text]\n",
        "\n",
        "\n",
        "def Stemming(data,stemmer):\n",
        "    stemmed_data = [' '.join([stemmer.stem(word) for word in sentence.split() if ((word not in stop_wordsR) or (word not in stop_wordsE))]) for sentence in data]\n",
        "    return stemmed_data# or (word not in string.punctuation)\n",
        "\n",
        "def tfidf_vectorize(data, tfidf):\n",
        "    X = tfidf.fit_transform(data)\n",
        "    return X\n",
        "\n",
        "morph = MorphAnalyzer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    lemmatized_tokens = []\n",
        "    for sentence in text:\n",
        "        tokens = word_tokenize(sentence)\n",
        "        lemmatized_tokens.append(' '.join([morph.parse(token)[0].normal_form for token in tokens]))\n",
        "    return lemmatized_tokens\n",
        "\n",
        "def word2vec_vectorize(data):\n",
        "    model = Word2Vec(data, window=5, min_count=1, workers=4)\n",
        "    vectorized_data = np.array([np.mean([model.wv[token] for token in sentence], axis=0) for sentence in data])\n",
        "    return vectorized_data\n",
        "\n",
        "def fast_text(texts):\n",
        "    # Преобразование строк в формат, подходящий для FastText\n",
        "    with open('data.txt', 'w', encoding='utf-8') as f:\n",
        "        for text in texts:\n",
        "            f.write(f'__label__1 {text}\\n')\n",
        "\n",
        "    # Обучение FastText модели\n",
        "    model = fasttext.train_supervised('data.txt')\n",
        "\n",
        "    # Получение векторов для каждой строки\n",
        "    vectors = [model.get_sentence_vector(text) for text in texts]\n",
        "    return vectors\n",
        "\n",
        "\n",
        "st1=SnowballStemmer(\"russian\")\n",
        "st2=SnowballStemmer(\"english\")\n",
        "\n",
        "for i in range(len(urld)):\n",
        "  urld[i]=urld[i].replace('.',' ')\n",
        "\n",
        "\n",
        "stem_data=Stemming(text,st1)\n",
        "stem_url=Stemming(urld,st2)\n",
        "\n",
        "#vectorised=fast_text(stem_data)\n",
        "#vectorised_url=fast_text(stem_url)\n",
        "\n",
        "#stem_data=lemmatize_text(text)\n",
        "#stem_url=lemmatize_text(urld)\n",
        "\n",
        "tfidf1 = TfidfVectorizer(ngram_range=(1,2))\n",
        "tfidf2 = TfidfVectorizer(ngram_range=(1,2))\n",
        "\n",
        "vectorised=tfidf_vectorize(stem_data,tfidf1)\n",
        "vectorised_url=tfidf_vectorize(stem_url,tfidf2)\n",
        "\n",
        "#vectorised=word2vec_vectorize(stem_data)\n",
        "#vectorised_url=word2vec_vectorize(stem_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Классификация"
      ],
      "metadata": {
        "id": "RCUaemnLAgC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=MultinomialNB()\n",
        "model_url=MultinomialNB()\n",
        "model.fit(vectorised,y_tr)\n",
        "model_url.fit(vectorised_url,y_tr)\n",
        "y_pred=model.predict_proba(vectorised)[:,0]\n",
        "y_pred_url=model_url.predict_proba(vectorised_url)[:,0]\n",
        "\n",
        "#Попытка применить вторую модель на предсказаниях двух\n",
        "mod_gen=LogisticRegression()\n",
        "mas=np.vstack((y_pred, y_pred_url)).T\n",
        "print(mas.shape)\n",
        "mod_gen.fit(mas,y_tr)\n",
        "pr=np.where(mod_gen.predict_proba(mas)>0.65,0,1)[:,0]\n",
        "print(f1_score(pr,y_tr))\n",
        "print(confusion_matrix(pr,y_tr))\n",
        "\n",
        "y_gen=np.where(((y_pred+y_pred_url)/2)>0.71, 0,1)\n",
        "#print(f1_score(y_tr, y_pred))\n",
        "#print(f1_score(y_tr, y_pred_url))\n",
        "print(f1_score(y_tr, y_gen))\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_gen,y_tr))"
      ],
      "metadata": {
        "id": "CoC636H_4bOS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0002050-3eaf-4fe0-dd43-59347f29be77"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(135309, 2)\n",
            "0.9814385848801178\n",
            "[[118380    403]\n",
            " [   214  16312]]\n",
            "0.981243059932175\n",
            "[[118336    367]\n",
            " [   258  16348]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submit"
      ],
      "metadata": {
        "id": "oAR4eytU-JET"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3t5VlPiBTkA",
        "outputId": "146763df-5d83-4e21-a111-311082bdcd86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID,label\n",
            "135309,0\n",
            "135310,0\n",
            "135311,0\n",
            "135312,1\n",
            "135313,0\n",
            "135314,0\n",
            "135315,0\n",
            "135316,0\n",
            "135317,0\n"
          ]
        }
      ],
      "source": [
        "X_test_title=test_df['title']\n",
        "urld=test_df['url']\n",
        "#for i in range(len(urld)):\n",
        "  #urld[i]=urld[i].replace('.',' ')\n",
        "stem_datap=Stemming(X_test_title,st1)\n",
        "stem_urlp=Stemming(urld,st2)\n",
        "\n",
        "vectorisedp=tfidf1.transform(stem_datap)\n",
        "vectorised_urlp=tfidf2.transform(stem_urlp)\n",
        "\n",
        "y_pred=model.predict_proba(vectorisedp)[:,0]\n",
        "y_pred_url=model_url.predict_proba(vectorised_urlp)[:,0]\n",
        "\n",
        "y_gen=np.where(((y_pred+y_pred_url)/2)>0.7, 0,1)\n",
        "#y_gen=np.where(((6*y_pred/5+4*y_pred_url/5)/2)>0.78, 0,1)\n",
        "#mas=np.vstack((y_pred, y_pred_url)).T\n",
        "#y_gen=np.where(mod_gen.predict_proba(mas)>0.65,0,1)[:,0]\n",
        "\n",
        "test_df[\"label\"] = y_gen\n",
        "\n",
        "test_df[[\"ID\", \"label\"]].to_csv(\"ml_baseline.csv\", index=False)\n",
        "\n",
        "!cat ml_baseline.csv | head"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попытка понижения размерности"
      ],
      "metadata": {
        "id": "92_fCANE-YLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "tsvd = TruncatedSVD(n_components=10)\n",
        "\n",
        "# apply the truncatedSVD function\n",
        "X_sparse_tsvd = tsvd.fit(vectorised).transform(vectorised)\n",
        "print(X_sparse_tsvd)\n",
        "\n",
        "model=LogisticRegression()\n",
        "model_url=MultinomialNB()\n",
        "model.fit(X_sparse_tsvd,y_tr)\n",
        "model_url.fit(vectorised_url,y_tr)\n",
        "y_pred=model.predict_proba(X_sparse_tsvd)[:,0]\n",
        "y_pred_url=model_url.predict_proba(vectorised_url)[:,0]\n",
        "#model_gen=RandomForestClassifier()\n",
        "#model_gen.fit(y_pred,y_pred_url,y_tr)\n",
        "#print(f1_score(model_gen.predict(y_pred,y_pred_url),y_tr))\n",
        "\n",
        "y_gen=np.where(((y_pred+y_pred_url)/2)>0.68, 0,1)\n",
        "#print(f1_score(y_tr, y_pred))\n",
        "#print(f1_score(y_tr, y_pred_url))\n",
        "print(f1_score(y_tr, y_gen))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcXGvie9OAfu",
        "outputId": "edcf3fa9-6012-4e35-a131-2680ea82c807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 3.21931635e-03 -6.73645054e-04  8.65154598e-03 ... -1.29926033e-02\n",
            "  -9.33028932e-03 -1.34207197e-02]\n",
            " [ 2.68387691e-03 -7.36425956e-04  1.20886223e-02 ... -1.97474279e-03\n",
            "   1.25340658e-02  3.10030774e-03]\n",
            " [ 1.83457838e-02 -2.27686916e-03  1.56622476e-01 ...  4.14038861e-02\n",
            "  -7.98423892e-02 -8.06254233e-03]\n",
            " ...\n",
            " [ 2.17170455e-04 -5.02625441e-05  1.18880020e-03 ... -7.16678704e-04\n",
            "  -8.28912443e-05  5.48399087e-03]\n",
            " [ 3.47115700e-03 -4.34124885e-04  4.76333215e-03 ... -4.93589395e-03\n",
            "  -7.07182183e-03  1.61380821e-02]\n",
            " [ 2.54227742e-03 -8.43560494e-04  3.67024821e-03 ... -4.46934720e-03\n",
            "  -9.90845001e-03  2.37969378e-02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попытка применить НС"
      ],
      "metadata": {
        "id": "ijgeDJex-oHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "trainX=torch.FloatTensor(X_sparse_tsvd)\n",
        "trainY=torch.FloatTensor(y_tr)\n",
        "\n",
        "class network(nn.Module):\n",
        "  def __init__(self,n_input, n_hidden):\n",
        "    super(network,self).__init__()\n",
        "    self.fc1=nn.Linear(n_input, n_hidden)\n",
        "    self.fc2=nn.Linear(n_hidden, n_hidden)\n",
        "    self.fc3=nn.Linear(n_hidden, 1)\n",
        "    self.act1=nn.ReLU()\n",
        "    self.act2=nn.ReLU()\n",
        "    self.sm=nn.Sigmoid()\n",
        "    #self.loss=nn.CrossEntropyLoss()\n",
        "    self.loss=nn.BCEWithLogitsLoss()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.fc1(x)\n",
        "    x=self.act1(x)\n",
        "    x=self.fc2(x)\n",
        "    x=self.act2(x)\n",
        "    x=self.fc3(x)\n",
        "    return x\n",
        "\n",
        "  def fit(self,trainX,trainY, epochs=100, batch_size=20):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=1e-2,betas=(0.9, 0.99))#, betas=(0.9, 0.99)\n",
        "    for epoch in range(epochs):\n",
        "      idx=np.random.permutation(trainX.shape[0]) #массив индексов зад. длины\n",
        "      for i in range(0,trainX.shape[0],batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        batch_ind=idx[i:i+batch_size]\n",
        "        x_batch=trainX[batch_ind]\n",
        "        y_batch=trainY[batch_ind]\n",
        "\n",
        "        y_pred=self.forward(x_batch).ravel()\n",
        "        loss_val=self.loss(y_pred, y_batch)\n",
        "        loss_val.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "  def predict(self, x):\n",
        "    x=self.forward(x)\n",
        "    x=self.sm(x)\n",
        "    return x\n",
        "\n",
        "#Net=network(trainX.shape[1],10)\n",
        "#Net.fit(trainX,trainY)\n",
        "n=1\n",
        "acc=0\n",
        "for i in range(trainY.shape[0]):\n",
        "  preds=(Net.predict(trainX[i]).detach().numpy())\n",
        "  preds=np.where(preds>0.4,1,0)\n",
        "  acc += ((preds==trainY[i].numpy())+y_pred_url[i])/2\n",
        "print(\"train accuracy: \",acc/trainY.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRFUhm-NJGc_",
        "outputId": "ec5948d6-52e1-4921-bdd9-a62f5991c436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train accuracy:  [0.90200401]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXJ4hfE_Gw10"
      },
      "outputs": [],
      "source": [
        "from  gensim.models.KeyedVectors import load_word2vec_format\n",
        "def load_word2vec():\n",
        "    word2vecDict = load_word2vec_format(\n",
        "        '../input/word2vec-google/GoogleNews-vectors-negative300.bin',\n",
        "        binary=True, unicode_errors='ignore')\n",
        "    embeddings_index = dict()\n",
        "    for word in word2vecDict.wv.vocab:\n",
        "        embeddings_index[word] = word2vecDict.word_vec(word)\n",
        "\n",
        "    return embeddings_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbGkrMH0tbgg"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "model_path = '/content/model.hdf5'\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(model_path)\n",
        "\n",
        "def vectorize_sentences(sentences):\n",
        "    vectorized_sentences = []\n",
        "    for sentence in sentences:\n",
        "        sentence_vector = []\n",
        "        words = sentence.split()\n",
        "        for word in words:\n",
        "            try:\n",
        "                word_vector = model[word]\n",
        "                sentence_vector.append(word_vector)\n",
        "            except KeyError:\n",
        "                # Если слово отсутствует в словаре, пропускаем его\n",
        "                pass\n",
        "        vectorized_sentences.append(np.mean(sentence_vector, axis=0))  # Усредняем векторы слов для получения одного вектора предложения\n",
        "    return np.array(vectorized_sentences)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}